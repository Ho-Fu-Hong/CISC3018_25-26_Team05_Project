{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/01 20:45:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "Number of rows: 45706\n",
      "Number of columns: 21\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import libraries and load data\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, GBTRegressor, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YieldDataAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the processed dataset\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/vivi/Downloads/yield_df.csv\")\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CORRELATION ANALYSIS\n",
      "============================================================\n",
      "Correlation with Yield (hg/ha_yield):\n",
      "Correlation between yield and average_rain_fall_mm_per_year      : -0.0279\n",
      "Correlation between yield and pesticides_tonnes                  : 0.1572\n",
      "Correlation between yield and avg_temp                           : -0.0584\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Rename columns and correlation analysis\n",
    "df = df.withColumnRenamed(\"Yield\", \"hg/ha_yield\") \\\n",
    "       .withColumnRenamed(\"Rainfall\", \"average_rain_fall_mm_per_year\") \\\n",
    "       .withColumnRenamed(\"Pesticides\", \"pesticides_tonnes\") \\\n",
    "       .withColumnRenamed(\"Avg_Temperature\", \"avg_temp\") \\\n",
    "       .withColumnRenamed(\"Country\", \"Area\") \\\n",
    "       .withColumnRenamed(\"Crop\", \"Item\")\n",
    "\n",
    "# Correlation Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "numeric_columns = ['hg/ha_yield', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']\n",
    "\n",
    "print(\"Correlation with Yield (hg/ha_yield):\")\n",
    "for col_name in numeric_columns:\n",
    "    if col_name != 'hg/ha_yield':\n",
    "        correlation = df.stat.corr('hg/ha_yield', col_name)\n",
    "        print(f\"Correlation between yield and {col_name:35}: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "REGRESSION ANALYSIS - Predicting Crop Yield\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data count: 36698\n",
      "Test data count: 9008\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data preparation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REGRESSION ANALYSIS - Predicting Crop Yield\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Handle categorical variables\n",
    "area_indexer = StringIndexer(inputCol=\"Area\", outputCol=\"AreaIndex\", handleInvalid=\"keep\")\n",
    "item_indexer = StringIndexer(inputCol=\"Item\", outputCol=\"ItemIndex\", handleInvalid=\"keep\")\n",
    "\n",
    "# Use OneHotEncoder for categorical features\n",
    "area_encoder = OneHotEncoder(inputCol=\"AreaIndex\", outputCol=\"AreaVec\")\n",
    "item_encoder = OneHotEncoder(inputCol=\"ItemIndex\", outputCol=\"ItemVec\")\n",
    "\n",
    "# Feature columns\n",
    "feature_columns = ['AreaVec', 'ItemVec', 'Year', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']\n",
    "\n",
    "# Split the data\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training data count: {train_data.count()}\")\n",
    "print(f\"Test data count: {test_data.count()}\")\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"hg/ha_yield\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"hg/ha_yield\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"hg/ha_yield\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. LINEAR REGRESSION (BASELINE)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 20:45:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R²: 0.7747\n",
      "Linear Regression MAE: 22380.59\n",
      "Linear Regression RMSE: 33423.60\n",
      "\n",
      "Linear Regression Coefficients (interpretation):\n",
      "Could not extract coefficients: [NOT_COLUMN_OR_STR] Argument `col` should be a Column or str, got float64.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Linear Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. LINEAR REGRESSION (BASELINE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\",\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0,\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "pipeline_lr = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, lr])\n",
    "lr_model = pipeline_lr.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "lr_r2 = evaluator_r2.evaluate(lr_predictions)\n",
    "lr_mae = evaluator_mae.evaluate(lr_predictions)\n",
    "lr_rmse = evaluator_rmse.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Linear Regression R²: {lr_r2:.4f}\")\n",
    "print(f\"Linear Regression MAE: {lr_mae:.2f}\")\n",
    "print(f\"Linear Regression RMSE: {lr_rmse:.2f}\")\n",
    "\n",
    "results['Linear Regression'] = {'r2': lr_r2, 'mae': lr_mae, 'rmse': lr_rmse}\n",
    "\n",
    "# Get coefficients for interpretation\n",
    "try:\n",
    "    lr_coefficients = lr_model.stages[-1].coefficients.toArray()\n",
    "    feature_names = ['Area', 'Item', 'Year', 'Rainfall', 'Pesticides', 'Temperature']\n",
    "    print(\"\\nLinear Regression Coefficients (interpretation):\")\n",
    "    for i, (name, coef) in enumerate(zip(feature_names, lr_coefficients)):\n",
    "        impact = \"increases\" if coef > 0 else \"decreases\"\n",
    "        coef_abs = abs(coef)  # Python's abs() not Spark's\n",
    "        print(f\"  {name:15}: {coef:.6f} (each unit {impact} yield by {coef_abs:.2f} hg/ha)\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not extract coefficients: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. RIDGE REGRESSION (L2 REGULARIZATION)\n",
      "============================================================\n",
      "Ridge Regression R²: 0.7747\n",
      "Ridge Regression MAE: 22380.58\n",
      "Ridge Regression RMSE: 33423.60\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Ridge Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. RIDGE REGRESSION (L2 REGULARIZATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ridge = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\",\n",
    "    elasticNetParam=0,\n",
    "    regParam=0.1,\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "pipeline_ridge = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, ridge])\n",
    "\n",
    "try:\n",
    "    ridge_model = pipeline_ridge.fit(train_data)\n",
    "    ridge_predictions = ridge_model.transform(test_data)\n",
    "    \n",
    "    ridge_r2 = evaluator_r2.evaluate(ridge_predictions)\n",
    "    ridge_mae = evaluator_mae.evaluate(ridge_predictions)\n",
    "    ridge_rmse = evaluator_rmse.evaluate(ridge_predictions)\n",
    "    \n",
    "    print(f\"Ridge Regression R²: {ridge_r2:.4f}\")\n",
    "    print(f\"Ridge Regression MAE: {ridge_mae:.2f}\")\n",
    "    print(f\"Ridge Regression RMSE: {ridge_rmse:.2f}\")\n",
    "    \n",
    "    results['Ridge Regression'] = {'r2': ridge_r2, 'mae': ridge_mae, 'rmse': ridge_rmse}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Ridge Regression failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. LASSO REGRESSION (L1 REGULARIZATION)\n",
      "============================================================\n",
      "Lasso Regression R²: 0.7747\n",
      "Lasso Regression MAE: 22380.58\n",
      "Lasso Regression RMSE: 33423.60\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Lasso Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. LASSO REGRESSION (L1 REGULARIZATION)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lasso = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\",\n",
    "    elasticNetParam=1,\n",
    "    regParam=0.01,\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "pipeline_lasso = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, lasso])\n",
    "\n",
    "try:\n",
    "    lasso_model = pipeline_lasso.fit(train_data)\n",
    "    lasso_predictions = lasso_model.transform(test_data)\n",
    "    \n",
    "    lasso_r2 = evaluator_r2.evaluate(lasso_predictions)\n",
    "    lasso_mae = evaluator_mae.evaluate(lasso_predictions)\n",
    "    lasso_rmse = evaluator_rmse.evaluate(lasso_predictions)\n",
    "    \n",
    "    print(f\"Lasso Regression R²: {lasso_r2:.4f}\")\n",
    "    print(f\"Lasso Regression MAE: {lasso_mae:.2f}\")\n",
    "    print(f\"Lasso Regression RMSE: {lasso_rmse:.2f}\")\n",
    "    \n",
    "    results['Lasso Regression'] = {'r2': lasso_r2, 'mae': lasso_mae, 'rmse': lasso_rmse}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Lasso Regression failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. ELASTIC NET REGRESSION\n",
      "============================================================\n",
      "Elastic Net R²: 0.7747\n",
      "Elastic Net MAE: 22380.47\n",
      "Elastic Net RMSE: 33423.60\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Elastic Net\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. ELASTIC NET REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "elastic_net = LinearRegression(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\",\n",
    "    elasticNetParam=0.5,\n",
    "    regParam=0.05,\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "pipeline_elastic = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, elastic_net])\n",
    "\n",
    "try:\n",
    "    elastic_model = pipeline_elastic.fit(train_data)\n",
    "    elastic_predictions = elastic_model.transform(test_data)\n",
    "    \n",
    "    elastic_r2 = evaluator_r2.evaluate(elastic_predictions)\n",
    "    elastic_mae = evaluator_mae.evaluate(elastic_predictions)\n",
    "    elastic_rmse = evaluator_rmse.evaluate(elastic_predictions)\n",
    "    \n",
    "    print(f\"Elastic Net R²: {elastic_r2:.4f}\")\n",
    "    print(f\"Elastic Net MAE: {elastic_mae:.2f}\")\n",
    "    print(f\"Elastic Net RMSE: {elastic_rmse:.2f}\")\n",
    "    \n",
    "    results['Elastic Net'] = {'r2': elastic_r2, 'mae': elastic_mae, 'rmse': elastic_rmse}\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Elastic Net failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. K-NEAREST NEIGHBORS REGRESSION (KNN)\n",
      "============================================================\n",
      "Running KNN analysis...\n",
      "Data converted: train=36698 rows, test=9008 rows\n",
      "KNN with k= 3: R²=0.9682, MAE=4150.71, RMSE=12555.67\n",
      "KNN with k= 5: R²=0.9540, MAE=5873.35, RMSE=15102.37\n",
      "KNN with k= 7: R²=0.9405, MAE=7126.25, RMSE=17174.77\n",
      "KNN with k=10: R²=0.9267, MAE=8442.56, RMSE=19060.57\n",
      "KNN with k=15: R²=0.9062, MAE=10195.44, RMSE=21560.81\n",
      "\n",
      "Best KNN: k=3, R²=0.9682, MAE=4150.71, RMSE=12555.67\n",
      "\n",
      "============================================================\n",
      "KNN Performance Summary\n",
      "============================================================\n",
      "K Value    R² Score     MAE          RMSE        \n",
      "------------------------------------------------------------\n",
      "3          0.9682       4150.71      12555.67    \n",
      "5          0.9540       5873.35      15102.37    \n",
      "7          0.9405       7126.25      17174.77    \n",
      "10         0.9267       8442.56      19060.57    \n",
      "15         0.9062       10195.44     21560.81    \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: K-Nearest Neighbors (KNN) \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. K-NEAREST NEIGHBORS REGRESSION (KNN)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(\"Running KNN analysis...\")\n",
    "\n",
    "# Convert Spark DataFrames to pandas\n",
    "train_pd = train_data.toPandas()\n",
    "test_pd = test_data.toPandas()\n",
    "print(f\"Data converted: train={len(train_pd)} rows, test={len(test_pd)} rows\")\n",
    "\n",
    "# Prepare features for KNN\n",
    "feature_cols_knn = ['Year', 'average_rain_fall_mm_per_year', 'pesticides_tonnes', 'avg_temp']\n",
    "X_train_knn = train_pd[feature_cols_knn].copy()\n",
    "X_test_knn = test_pd[feature_cols_knn].copy()\n",
    "\n",
    "# Add encoded categorical variables\n",
    "categorical_cols = ['Area', 'Item']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # Fill missing values\n",
    "    train_pd[col] = train_pd[col].fillna('Unknown')\n",
    "    test_pd[col] = test_pd[col].fillna('Unknown')\n",
    "    \n",
    "    # Create and fit label encoder\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_pd[col], test_pd[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    \n",
    "    # Transform the data\n",
    "    X_train_knn[col] = le.transform(train_pd[col])\n",
    "    X_test_knn[col] = le.transform(test_pd[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Handle missing values in numerical features\n",
    "for col in feature_cols_knn:\n",
    "    if col in X_train_knn.columns:\n",
    "        X_train_knn[col] = X_train_knn[col].fillna(X_train_knn[col].mean())\n",
    "        X_test_knn[col] = X_test_knn[col].fillna(X_train_knn[col].mean())\n",
    "\n",
    "# Scale features (important for KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_knn)\n",
    "X_test_scaled = scaler.transform(X_test_knn)\n",
    "\n",
    "# Target variable\n",
    "y_train = train_pd['hg/ha_yield'].fillna(train_pd['hg/ha_yield'].mean())\n",
    "y_test = test_pd['hg/ha_yield'].fillna(test_pd['hg/ha_yield'].mean())\n",
    "\n",
    "# Try different K values\n",
    "k_values = [3, 5, 7, 10, 15]\n",
    "knn_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    # Create and train KNN model\n",
    "    knn = KNeighborsRegressor(n_neighbors=k, weights='distance', n_jobs=-1)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    # Store results\n",
    "    knn_results[k] = {'r2': r2, 'mae': mae, 'rmse': rmse}\n",
    "    \n",
    "    # Print results for this k\n",
    "    print(f\"KNN with k={k:2d}: R²={r2:.4f}, MAE={mae:.2f}, RMSE={rmse:.2f}\")\n",
    "\n",
    "# Find best K based on highest R² score\n",
    "best_k = None\n",
    "best_r2 = -float('inf')\n",
    "best_mae = None\n",
    "best_rmse = None\n",
    "\n",
    "for k, metrics in knn_results.items():\n",
    "    if metrics['r2'] > best_r2:\n",
    "        best_r2 = metrics['r2']\n",
    "        best_k = k\n",
    "        best_mae = metrics['mae']\n",
    "        best_rmse = metrics['rmse']\n",
    "\n",
    "# Print best results\n",
    "print(f\"\\nBest KNN: k={best_k}, R²={best_r2:.4f}, MAE={best_mae:.2f}, RMSE={best_rmse:.2f}\")\n",
    "\n",
    "# Store results in results dictionary\n",
    "results[f'KNN (k={best_k})'] = {'r2': best_r2, 'mae': best_mae, 'rmse': best_rmse}\n",
    "\n",
    "# Optional: Visualize KNN performance\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KNN Performance Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'K Value':<10} {'R² Score':<12} {'MAE':<12} {'RMSE':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for k in sorted(k_values):\n",
    "    metrics = knn_results[k]\n",
    "    print(f\"{k:<10} {metrics['r2']:<12.4f} {metrics['mae']:<12.2f} {metrics['rmse']:<12.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. DECISION TREE REGRESSOR\n",
      "============================================================\n",
      "Distinct areas: 116\n",
      "Distinct crops: 10\n",
      "Setting maxBins to: 117\n",
      "Decision Tree R²: 0.9222\n",
      "Decision Tree MAE: 11034.37\n",
      "Decision Tree RMSE: 19643.19\n",
      "\n",
      "Decision Tree Feature Importances:\n",
      "  Year           : 0.0317\n",
      "  Item           : 0.0032\n",
      "  Pesticides     : 0.0017\n",
      "  Area           : 0.0003\n",
      "  Rainfall       : 0.0000\n",
      "  Temperature    : 0.0000\n",
      "\n",
      "Decision Tree Details:\n",
      "  Depth: 10\n",
      "  Number of nodes: 1221\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Decision Tree Regressor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. DECISION TREE REGRESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "distinct_areas = df.select(\"Area\").distinct().count()\n",
    "distinct_items = df.select(\"Item\").distinct().count()\n",
    "# Use custom logic instead of max() to avoid conflict with PySpark's max function\n",
    "max_bins_candidates = [distinct_areas + 1, distinct_items + 1, 64]\n",
    "max_bins_value = max_bins_candidates[0]\n",
    "for candidate in max_bins_candidates[1:]:\n",
    "    if candidate > max_bins_value:\n",
    "        max_bins_value = candidate\n",
    "\n",
    "print(f\"Distinct areas: {distinct_areas}\")\n",
    "print(f\"Distinct crops: {distinct_items}\")\n",
    "print(f\"Setting maxBins to: {max_bins_value}\")\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\", \n",
    "    seed=42, \n",
    "    maxDepth=10,\n",
    "    maxBins=max_bins_value\n",
    ")\n",
    "\n",
    "pipeline_dt = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, dt])\n",
    "\n",
    "try:\n",
    "    dt_model = pipeline_dt.fit(train_data)\n",
    "    dt_predictions = dt_model.transform(test_data)\n",
    "    \n",
    "    dt_r2 = evaluator_r2.evaluate(dt_predictions)\n",
    "    dt_mae = evaluator_mae.evaluate(dt_predictions)\n",
    "    dt_rmse = evaluator_rmse.evaluate(dt_predictions)\n",
    "    \n",
    "    print(f\"Decision Tree R²: {dt_r2:.4f}\")\n",
    "    print(f\"Decision Tree MAE: {dt_mae:.2f}\")\n",
    "    print(f\"Decision Tree RMSE: {dt_rmse:.2f}\")\n",
    "    \n",
    "    results['Decision Tree'] = {'r2': dt_r2, 'mae': dt_mae, 'rmse': dt_rmse}\n",
    "    \n",
    "    # Feature Importance\n",
    "    dt_feature_importances = dt_model.stages[-1].featureImportances\n",
    "    \n",
    "    # Create simplified feature importance names\n",
    "    feature_names = ['Area', 'Item', 'Year', 'Rainfall', 'Pesticides', 'Temperature']\n",
    "    dt_feature_importance_dict = {}\n",
    "    for i, name in enumerate(feature_names):\n",
    "        if i < len(dt_feature_importances):\n",
    "            dt_feature_importance_dict[name] = dt_feature_importances[i]\n",
    "        else:\n",
    "            dt_feature_importance_dict[name] = 0.0\n",
    "    \n",
    "    print(\"\\nDecision Tree Feature Importances:\")\n",
    "    dt_items = list(dt_feature_importance_dict.items())\n",
    "    # Manual bubble sort\n",
    "    for i in range(len(dt_items)):\n",
    "        for j in range(i + 1, len(dt_items)):\n",
    "            if dt_items[i][1] < dt_items[j][1]:\n",
    "                dt_items[i], dt_items[j] = dt_items[j], dt_items[i]\n",
    "    \n",
    "    for feature, importance in dt_items:\n",
    "        print(f\"  {feature:15}: {importance:.4f}\")\n",
    "        \n",
    "    # Get Decision Tree model details\n",
    "    dt_model_final = dt_model.stages[-1]\n",
    "    print(f\"\\nDecision Tree Details:\")\n",
    "    print(f\"  Depth: {dt_model_final.depth}\")\n",
    "    print(f\"  Number of nodes: {dt_model_final.numNodes}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Decision Tree failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. RANDOM FOREST REGRESSOR\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/01 20:46:25 WARN DAGScheduler: Broadcasting large task binary with size 1177.2 KiB\n",
      "25/12/01 20:46:26 WARN DAGScheduler: Broadcasting large task binary with size 1835.4 KiB\n",
      "25/12/01 20:46:27 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "25/12/01 20:46:28 WARN DAGScheduler: Broadcasting large task binary with size 3.9 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest R²: 0.9176\n",
      "Random Forest MAE: 12095.49\n",
      "Random Forest RMSE: 20215.47\n",
      "\n",
      "Random Forest Feature Importances:\n",
      "  Year           : 0.0221)\n",
      "  Item           : 0.0121)\n",
      "  Area           : 0.0094)\n",
      "  Rainfall       : 0.0023)\n",
      "  Pesticides     : 0.0013)\n",
      "  Temperature    : 0.0013)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Random Forest Regressor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\", \n",
    "    seed=42, \n",
    "    maxDepth=10,\n",
    "    maxBins=max_bins_value,\n",
    "    numTrees=50\n",
    ")\n",
    "\n",
    "pipeline_rf = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, rf])\n",
    "\n",
    "try:\n",
    "    rf_model = pipeline_rf.fit(train_data)\n",
    "    rf_predictions = rf_model.transform(test_data)\n",
    "    \n",
    "    rf_r2 = evaluator_r2.evaluate(rf_predictions)\n",
    "    rf_mae = evaluator_mae.evaluate(rf_predictions)\n",
    "    rf_rmse = evaluator_rmse.evaluate(rf_predictions)\n",
    "    \n",
    "    print(f\"Random Forest R²: {rf_r2:.4f}\")\n",
    "    print(f\"Random Forest MAE: {rf_mae:.2f}\")\n",
    "    print(f\"Random Forest RMSE: {rf_rmse:.2f}\")\n",
    "    \n",
    "    results['Random Forest'] = {'r2': rf_r2, 'mae': rf_mae, 'rmse': rf_rmse}\n",
    "    \n",
    "    # Feature Importance from Random Forest\n",
    "    rf_feature_importances = rf_model.stages[-1].featureImportances\n",
    "    rf_feature_importance_dict = dict(zip(feature_names, rf_feature_importances))\n",
    "    \n",
    "    print(\"\\nRandom Forest Feature Importances:\")\n",
    "    rf_items = list(rf_feature_importance_dict.items())\n",
    "    for i in range(len(rf_items)):\n",
    "        for j in range(i + 1, len(rf_items)):\n",
    "            if rf_items[i][1] < rf_items[j][1]:\n",
    "                rf_items[i], rf_items[j] = rf_items[j], rf_items[i]\n",
    "    \n",
    "    for feature, importance in rf_items:\n",
    "        print(f\"  {feature:15}: {importance:.4f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Random Forest failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "8. GRADIENT BOOSTING REGRESSOR\n",
      "============================================================\n",
      "Gradient Boosting R²: 0.9408\n",
      "Gradient Boosting MAE: 10113.11\n",
      "Gradient Boosting RMSE: 17136.24\n",
      "\n",
      "Gradient Boosting Feature Importances:\n",
      "  Item           : 0.0175)\n",
      "  Year           : 0.0157)\n",
      "  Temperature    : 0.0041)\n",
      "  Pesticides     : 0.0021)\n",
      "  Rainfall       : 0.0012)\n",
      "  Area           : 0.0010)\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Gradient Boosting Regressor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"8. GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"hg/ha_yield\", \n",
    "    seed=42, \n",
    "    maxIter=50,\n",
    "    maxDepth=6,\n",
    "    maxBins=max_bins_value\n",
    ")\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=[area_indexer, item_indexer, area_encoder, item_encoder, assembler, gbt])\n",
    "\n",
    "try:\n",
    "    gbt_model = pipeline_gbt.fit(train_data)\n",
    "    gbt_predictions = gbt_model.transform(test_data)\n",
    "    \n",
    "    gbt_r2 = evaluator_r2.evaluate(gbt_predictions)\n",
    "    gbt_mae = evaluator_mae.evaluate(gbt_predictions)\n",
    "    gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)\n",
    "    \n",
    "    print(f\"Gradient Boosting R²: {gbt_r2:.4f}\")\n",
    "    print(f\"Gradient Boosting MAE: {gbt_mae:.2f}\")\n",
    "    print(f\"Gradient Boosting RMSE: {gbt_rmse:.2f}\")\n",
    "    \n",
    "    results['Gradient Boosting'] = {'r2': gbt_r2, 'mae': gbt_mae, 'rmse': gbt_rmse}\n",
    "    \n",
    "    # Feature Importance from Gradient Boosting\n",
    "    gbt_feature_importances = gbt_model.stages[-1].featureImportances\n",
    "    gbt_feature_importance_dict = dict(zip(feature_names, gbt_feature_importances))\n",
    "    \n",
    "    print(\"\\nGradient Boosting Feature Importances:\")\n",
    "    gbt_items = list(gbt_feature_importance_dict.items())\n",
    "    for i in range(len(gbt_items)):\n",
    "        for j in range(i + 1, len(gbt_items)):\n",
    "            if gbt_items[i][1] < gbt_items[j][1]:\n",
    "                gbt_items[i], gbt_items[j] = gbt_items[j], gbt_items[i]\n",
    "    \n",
    "    for feature, importance in gbt_items:\n",
    "        print(f\"  {feature:15}: {importance:.4f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Gradient Boosting failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "================================================================================\n",
      "Model                     | R²         | MAE          | RMSE         | Rank  \n",
      "--------------------------------------------------------------------------------\n",
      "KNN (k=3)                 |     0.9682 |      4150.71 |     12555.67 |      1\n",
      "Gradient Boosting         |     0.9408 |     10113.11 |     17136.24 |      2\n",
      "Decision Tree             |     0.9222 |     11034.37 |     19643.19 |      3\n",
      "Random Forest             |     0.9176 |     12095.49 |     20215.47 |      4\n",
      "Ridge Regression          |     0.7747 |     22380.58 |     33423.60 |      5\n",
      "Linear Regression         |     0.7747 |     22380.59 |     33423.60 |      6\n",
      "Elastic Net               |     0.7747 |     22380.47 |     33423.60 |      7\n",
      "Lasso Regression          |     0.7747 |     22380.58 |     33423.60 |      8\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Comprehensive Model Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"{'Model':25} | {'R²':10} | {'MAE':12} | {'RMSE':12} | {'Rank':6}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Sort models by R² score\n",
    "sorted_results = []\n",
    "for model_name, metrics in results.items():\n",
    "    sorted_results.append((model_name, metrics['r2'], metrics['mae'], metrics['rmse']))\n",
    "\n",
    "# Manual sorting (bubble sort)\n",
    "for i in range(len(sorted_results)):\n",
    "    for j in range(i + 1, len(sorted_results)):\n",
    "        if sorted_results[i][1] < sorted_results[j][1]:\n",
    "            sorted_results[i], sorted_results[j] = sorted_results[j], sorted_results[i]\n",
    "\n",
    "# Display results with ranking\n",
    "for rank, (model_name, r2, mae, rmse) in enumerate(sorted_results, 1):\n",
    "    print(f\"{model_name:25} | {r2:10.4f} | {mae:12.2f} | {rmse:12.2f} | {rank:6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE INTERPRETATION\n",
      "================================================================================\n",
      "EXCEPTIONAL PERFORMANCE - KNN (k=3) achieved R² = 0.9682\n",
      "   This means the model explains 96.8% of the variance in crop yields\n",
      "   This is EXCEPTIONAL predictive power for agricultural data!\n",
      "   The model is highly reliable for precision agriculture applications\n",
      "\n",
      "ADDITIONAL PERFORMANCE METRICS:\n",
      "   Mean Absolute Error (MAE): 4150.71 hg/ha\n",
      "   Root Mean Squared Error (RMSE): 12555.67 hg/ha\n",
      "   Mean Yield (baseline): 75381.22 hg/ha\n",
      "   MAE relative to mean: 5.5%\n",
      "   RMSE relative to mean: 16.7%\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Performance Interpretation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE INTERPRETATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = sorted_results[0][0]\n",
    "best_r2 = sorted_results[0][1]\n",
    "best_mae = sorted_results[0][2]\n",
    "best_rmse = sorted_results[0][3]\n",
    "\n",
    "# Calculate mean yield for context\n",
    "mean_yield = df.agg(mean('hg/ha_yield')).collect()[0][0]\n",
    "\n",
    "if best_r2 > 0.9:\n",
    "    print(f\"EXCEPTIONAL PERFORMANCE - {best_model_name} achieved R² = {best_r2:.4f}\")\n",
    "    print(f\"   This means the model explains {best_r2*100:.1f}% of the variance in crop yields\")\n",
    "    print(\"   This is EXCEPTIONAL predictive power for agricultural data!\")\n",
    "    print(\"   The model is highly reliable for precision agriculture applications\")\n",
    "elif best_r2 > 0.7:\n",
    "    print(f\"EXCELLENT PERFORMANCE - {best_model_name} achieved R² = {best_r2:.4f}\")\n",
    "    print(f\"   This means the model explains {best_r2*100:.1f}% of the variance in crop yields\")\n",
    "    print(\"   This is EXCELLENT predictive power for agricultural data!\")\n",
    "    print(\"   The model is highly suitable for yield prediction and forecasting\")\n",
    "elif best_r2 > 0.5:\n",
    "    print(f\"GOOD PERFORMANCE - {best_model_name} achieved R² = {best_r2:.4f}\")\n",
    "    print(f\"   This means the model explains {best_r2*100:.1f}% of the variance in crop yields\")\n",
    "    print(\"   This is GOOD predictive power for agricultural data!\")\n",
    "    print(\"   The model has moderate predictive capability for yield estimation\")\n",
    "elif best_r2 > 0.3:\n",
    "    print(f\"MODERATE PERFORMANCE - {best_model_name} achieved R² = {best_r2:.4f}\")\n",
    "    print(f\"   This means the model explains {best_r2*100:.1f}% of the variance in crop yields\")\n",
    "    print(\"   This is MODERATE predictive power for agricultural data\")\n",
    "    print(\"   Consider additional features or data for better performance\")\n",
    "else:\n",
    "    print(f\"POOR PERFORMANCE - {best_model_name} achieved R² = {best_r2:.4f}\")\n",
    "    print(f\"   This means the model explains only {best_r2*100:.1f}% of the variance in crop yields\")\n",
    "    print(\"   This is POOR predictive power for agricultural data\")\n",
    "    print(\"   Major improvements needed in feature engineering or data collection\")\n",
    "\n",
    "# Additional performance metrics\n",
    "print(f\"\\nADDITIONAL PERFORMANCE METRICS:\")\n",
    "print(f\"   Mean Absolute Error (MAE): {best_mae:.2f} hg/ha\")\n",
    "print(f\"   Root Mean Squared Error (RMSE): {best_rmse:.2f} hg/ha\")\n",
    "print(f\"   Mean Yield (baseline): {mean_yield:.2f} hg/ha\")\n",
    "print(f\"   MAE relative to mean: {(best_mae/mean_yield*100):.1f}%\")\n",
    "print(f\"   RMSE relative to mean: {(best_rmse/mean_yield*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET SUMMARY\n",
      "================================================================================\n",
      "• Total records: 45,706\n",
      "• Countries/Areas: 116\n",
      "• Crops/Items: 10\n",
      "• Years covered: 23\n",
      "• Mean yield: 75381.22 hg/ha\n",
      "• Yield range: 50 - 347555 hg/ha\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Dataset Summary and Cleanup\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "distinct_areas = df.select(\"Area\").distinct().count()\n",
    "distinct_items = df.select(\"Item\").distinct().count()\n",
    "distinct_years = df.select(\"Year\").distinct().count()\n",
    "min_yield = df.agg(min('hg/ha_yield')).collect()[0][0]\n",
    "max_yield = df.agg(max('hg/ha_yield')).collect()[0][0]\n",
    "\n",
    "print(f\"• Total records: {df.count():,}\")\n",
    "print(f\"• Countries/Areas: {distinct_areas}\")\n",
    "print(f\"• Crops/Items: {distinct_items}\")\n",
    "print(f\"• Years covered: {distinct_years}\")\n",
    "print(f\"• Mean yield: {mean_yield:.2f} hg/ha\")\n",
    "print(f\"• Yield range: {min_yield:.0f} - {max_yield:.0f} hg/ha\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark-env]",
   "language": "python",
   "name": "conda-env-pyspark-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
